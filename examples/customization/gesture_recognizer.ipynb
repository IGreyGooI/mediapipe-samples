{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-9BpIlqAZci"
   },
   "source": [
    "Project: /mediapipe/_project.yaml\n",
    "Book: /mediapipe/_book.yaml\n",
    "\n",
    "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
    "\n",
    "# Hand gesture recognition model customization guide\n",
    "\n",
    "<table align=\"left\" class=\"buttons\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T19:54:19.922801600Z",
     "start_time": "2025-02-12T19:54:19.735323Z"
    },
    "id": "JO1GUwC1_T2x"
   },
   "outputs": [],
   "source": [
    "#@title License information\n",
    "# Copyright 2023 The MediaPipe Authors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFBcmjzf0JLE"
   },
   "source": [
    "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
    "\n",
    "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGM0PT490LiR"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVVxZNfo0M0y"
   },
   "source": [
    "Install the MediaPipe Model Maker package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:46:22.488329400Z",
     "start_time": "2025-02-12T20:46:11.986728600Z"
    },
    "id": "6DBLRE-fqlO5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (25.0.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mediapipe-model-maker in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (0.2.1.4)\n",
      "Requirement already satisfied: absl-py in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (1.4.0)\n",
      "Requirement already satisfied: mediapipe>=0.10.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.10.21)\n",
      "Requirement already satisfied: numpy in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.10 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-addons in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.23.0)\n",
      "Requirement already satisfied: tensorflow-datasets in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (4.9.7)\n",
      "Requirement already satisfied: tensorflow-hub in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.16.1)\n",
      "Requirement already satisfied: tensorflow-model-optimization<0.8.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.7.5)\n",
      "Requirement already satisfied: tensorflow-text in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: tf-models-official<2.16.0,>=2.13.2 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (25.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (25.2.10)\n",
      "Requirement already satisfied: jax in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.34)\n",
      "Requirement already satisfied: jaxlib in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.34)\n",
      "Requirement already satisfied: matplotlib in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.10.0)\n",
      "Requirement already satisfied: opencv-contrib-python in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.4.0)\n",
      "Requirement already satisfied: packaging in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (24.2)\n",
      "Requirement already satisfied: setuptools in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-model-optimization<0.8.0->mediapipe-model-maker) (0.1.9)\n",
      "Requirement already satisfied: Cython in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.0.12)\n",
      "Requirement already satisfied: Pillow in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (11.1.0)\n",
      "Requirement already satisfied: gin-config in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.160.0)\n",
      "Requirement already satisfied: immutabledict in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.2.1)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.6.17)\n",
      "Requirement already satisfied: oauth2client in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: pandas>=0.22.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (9.0.0)\n",
      "Requirement already satisfied: pycocotools in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.0.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.0.2)\n",
      "Requirement already satisfied: sacrebleu in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.5.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.15.1)\n",
      "Requirement already satisfied: seqeval in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.2.2)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.1.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-hub->mediapipe-model-maker) (2.15.1)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-addons->mediapipe-model-maker) (2.13.3)\n",
      "Requirement already satisfied: click in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.8)\n",
      "Requirement already satisfied: promise in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
      "Requirement already satisfied: pyarrow in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (19.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (1.13.1)\n",
      "Requirement already satisfied: toml in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (4.67.1)\n",
      "Requirement already satisfied: array-record>=0.5.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.6.0)\n",
      "Requirement already satisfied: etils>=1.6.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.45.1)\n",
      "Requirement already satisfied: fsspec in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (6.5.2)\n",
      "Requirement already satisfied: zipp in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (3.21.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.24.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.1)\n",
      "Requirement already satisfied: bleach in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.2.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.10)\n",
      "Requirement already satisfied: CFFI>=1.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.17.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.2.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.9)\n",
      "Requirement already satisfied: portalocker in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.1.1)\n",
      "Requirement already satisfied: regex in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.9.0)\n",
      "Requirement already satisfied: colorama in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.6)\n",
      "Requirement already satisfied: lxml in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.6.1)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets->mediapipe-model-maker) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.67.0)\n",
      "Requirement already satisfied: pycparser in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.22)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.26.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.5.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.0.2)\n",
      "Requirement already satisfied: webencodings in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install mediapipe-model-maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3CvTNmB1WiY"
   },
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T19:54:47.624423900Z",
     "start_time": "2025-02-12T19:54:47.571171400Z"
    },
    "id": "c74UL9oI0VKU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:39:33.891895: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 11:39:33.891978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 11:39:33.893124: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 11:39:33.901037: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 11:39:45.321085: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "import os\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from mediapipe_model_maker import gesture_recognizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IppoENBmAuFn"
   },
   "source": [
    "## Simple End-to-End Example\n",
    "\n",
    "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8fMLXTdD6tW"
   },
   "source": [
    "### Get the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TwDFilngzjs"
   },
   "source": [
    "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
    "\n",
    "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.913201500Z"
    },
    "id": "6dwmyg5MnR_y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-02-13 11:40:34--  https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip\n",
      "198.18.3.75torage.googleapis.com (storage.googleapis.com)... \n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|198.18.3.75|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12332447 (12M) [application/zip]\n",
      "Saving to: ‘./.training-data/rps_data_sample.zip’\n",
      "\n",
      "./.training-data/rp 100%[===================>]  11.76M  9.25MB/s    in 1.3s    \n",
      "\n",
      "2025-02-13 11:40:36 (9.25 MB/s) - ‘./.training-data/rps_data_sample.zip’ saved [12332447/12332447]\n",
      "\n",
      "Archive:  ./.training-data/rps_data_sample.zip\n",
      "  inflating: ./.training-data/rps_data_sample/paper/77.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/837.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/176.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/406.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/771.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/89.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/76.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/74.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/439.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/411.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/955.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/941.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/170.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/158.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/206.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/944.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/575.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/401.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/165.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/818.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/72.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/832.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/601.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/628.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/166.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/465.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/539.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/505.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/935.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/504.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/276.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/302.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/666.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/855.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/658.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/472.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/249.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/103.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/846.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/891.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/477.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/339.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/701.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/258.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/884.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/890.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/847.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/879.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/306.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/460.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/299.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/919.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/717.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/918.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/529.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/461.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/887.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/677.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/844.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/518.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/901.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/531.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/690.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/321.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/282.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/269.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/533.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/719.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/730.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/283.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/37.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/654.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/898.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/495.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/318.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/278.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/734.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/537.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/319.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/655.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/680.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/18.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/24.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/657.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/119.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/482.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/284.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/905.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/483.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/865.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/25.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/194.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/143.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/744.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/977.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/142.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/80.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/57.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/7.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/183.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/395.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/544.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/236.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/785.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/791.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/394.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/54.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/192.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/838.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/50.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/179.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/227.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/757.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/971.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/811.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/51.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/53.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/146.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/620.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/393.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/973.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/594.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/423.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/91.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/paper/806.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/88.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/348.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/360.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/764.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/771.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/942.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/956.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/567.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/407.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/834.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/363.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/565.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/772.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/216.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/376.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/399.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/372.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/428.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/560.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/206.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/548.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/945.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/776.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/549.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/398.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/165.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/617.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/818.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/211.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/239.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/760.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/238.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/416.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/614.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/854.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/28.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/868.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/302.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/672.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/896.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/894.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/328.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/300.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/922.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/513.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/507.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/329.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/675.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/891.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/305.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/728.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/648.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/851.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/931.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/529.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/449.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/850.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/861.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/875.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/524.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/915.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/901.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/929.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/733.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/309.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/254.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/320.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/687.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/318.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/244.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/536.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/709.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/906.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/537.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/523.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/286.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/133.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/872.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/32.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/30.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/441.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/290.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/938.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/904.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/722.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/905.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/285.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/56.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/625.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/427.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/750.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/778.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/546.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/234.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/426.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/168.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/82.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/356.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/593.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/784.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/223.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/545.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/592.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/431.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/182.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/54.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/186.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/435.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/409.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/541.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/45.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/193.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/47.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/1.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/152.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/595.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/218.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/967.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/769.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/796.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/351.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/392.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/635.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/147.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/rock/621.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/604.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/162.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/610.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/764.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/942.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/188.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/60.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/149.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/559.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/798.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/954.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/968.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/49.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/819.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/602.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/400.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/428.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/788.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/549.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/617.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/64.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/70.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/173.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/239.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/760.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/589.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/614.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/14.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/28.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/129.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/673.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/707.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/706.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/276.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/289.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/100.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/29.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/116.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/670.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/705.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/261.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/301.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/117.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/856.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/16.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/463.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/715.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/927.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/264.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/338.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/476.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/853.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/104.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/306.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/448.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/925.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/930.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/878.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/691.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/732.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/531.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/323.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/684.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/321.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/269.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/527.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/730.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/122.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/23.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/126.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/293.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/522.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/278.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/720.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/251.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/641.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/866.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/657.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/119.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/482.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/520.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/534.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/285.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/656.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/871.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/56.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/631.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/382.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/792.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/208.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/397.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/156.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/630.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/5.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/57.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/803.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/7.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/96.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/222.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/550.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/785.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/791.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/753.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/223.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/394.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/169.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/196.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/800.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/192.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/50.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/804.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/596.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/568.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/232.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/45.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/90.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/387.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/436.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/378.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/595.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/230.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/782.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/580.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/147.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/scissors/184.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/823.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1031.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/957.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/229.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/598.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/349.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1811.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/76.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/60.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/820.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/613.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/363.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/203.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1224.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/955.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/941.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1033.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1812.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1609.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/71.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/65.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1802.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1590.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/993.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/950.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1552.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/213.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/165.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1340.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1424.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1381.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/239.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/946.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1784.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1747.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1814.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1627.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1633.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/9.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/511.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1244.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1906.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1469.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1119.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1643.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1657.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1521.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/704.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1534.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/275.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/249.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1134.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1083.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/729.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/270.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1041.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/39.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1876.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1255.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1650.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1308.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1446.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1488.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1885.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1107.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1729.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1067.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1072.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1890.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1851.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/255.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1299.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1501.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/334.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1878.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1663.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/122.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/136.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/33.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/898.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1101.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1883.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/735.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1048.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/292.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1712.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1128.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/32.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/657.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/327.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1843.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1089.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/939.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/520.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/252.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/802.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/433.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/590.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1206.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/779.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/744.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1005.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/142.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/57.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1198.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/424.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/236.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/791.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/394.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1416.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/800.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1412.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/44.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1823.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1189.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1214.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/794.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/756.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/193.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1799.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1000.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/386.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1362.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/190.jpg  \n",
      "  inflating: ./.training-data/rps_data_sample/none/1376.jpg  \n"
     ]
    }
   ],
   "source": [
    "!mkdir -p \"./.training-data\"\n",
    "!wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip -O \"./.training-data/rps_data_sample.zip\"\n",
    "!unzip -o \"./.training-data/rps_data_sample.zip\" -d \"./.training-data\"\n",
    "dataset_path = \"./.training-data/rps_data_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiWb9Tu3lBBI"
   },
   "source": [
    "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.914474700Z"
    },
    "id": "QgadM4VDj3Y2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./.training-data/rps_data_sample\n",
      "['none', 'paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "print(dataset_path)\n",
    "labels = []\n",
    "for i in os.listdir(dataset_path):\n",
    "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
    "    labels.append(i)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA0o59OMjqmV"
   },
   "source": [
    "To better understand the dataset, plot a couple of example images for each gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.916574300Z"
    },
    "id": "sx8PsrwYjvgO"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21670/544766458.py:13: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "\n",
    "for label in labels:\n",
    "  label_dir = os.path.join(dataset_path, label)\n",
    "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
    "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
    "  for i in range(NUM_EXAMPLES):\n",
    "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    axs[i].get_yaxis().set_visible(False)\n",
    "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWXwEXSXlg7d"
   },
   "source": [
    "### Run the example\n",
    "The workflow consists of 4 steps which have been separated into their own code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF9ArLQXIu25"
   },
   "source": [
    "**Load the dataset**\n",
    "\n",
    "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
    "\n",
    "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
    "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
    "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
    "\n",
    "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.918178600Z"
    },
    "id": "aTTNZsolKXiT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/339.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/730.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/162.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/329.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739418068.903005   21670 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1739418068.960107   21980 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: D3D12 (NVIDIA GeForce GTX 1080 Ti)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1739418069.004537   21982 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739418069.023862   21993 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739418069.085433   22011 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/522.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/593.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1883.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/476.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1814.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/394.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/705.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/706.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/771.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/595.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/782.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/666.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/117.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/227.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/879.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/575.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/844.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1119.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/719.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/942.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/687.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/119.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/950.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/865.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/285.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/387.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/57.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/614.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/82.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/942.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/894.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1747.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/433.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/854.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/376.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1089.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/186.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/293.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/819.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/483.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/407.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/7.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/730.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/784.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/338.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/65.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/461.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/709.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/448.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/269.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/28.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/757.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1416.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/208.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/80.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/838.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/39.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/136.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/190.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/146.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1643.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/431.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/37.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1627.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/672.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/54.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1381.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/472.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/520.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/258.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/617.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/25.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/89.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/234.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/193.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/779.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/393.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1299.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/802.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/72.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/967.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/71.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/955.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/938.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/684.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1885.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/270.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/898.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/594.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/44.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/915.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/602.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/568.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/495.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/18.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/760.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/64.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/853.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1067.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/680.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/306.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/284.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1784.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/925.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1501.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/179.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/971.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/477.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/206.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/628.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/539.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/283.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1906.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1663.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/524.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/56.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1534.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1376.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/531.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/142.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/348.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/143.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/147.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/534.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/939.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/614.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1224.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/152.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/45.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1128.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/7.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/239.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/760.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/728.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/617.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/147.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/673.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/613.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/398.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/423.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/482.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/211.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/192.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/549.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/428.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/32.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/306.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1340.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/798.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/436.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/931.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/45.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1469.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/50.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/166.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/194.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/321.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/956.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1000.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/334.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/427.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/504.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/88.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1048.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/238.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/656.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/328.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/631.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1214.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/463.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/901.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/690.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/904.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/846.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/735.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/691.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1255.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1812.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/590.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/531.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/275.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/394.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/545.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/589.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/222.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/734.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/320.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1041.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/993.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/744.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/50.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/596.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/598.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/278.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/406.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1712.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/269.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/945.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/559.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1033.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/957.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/165.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1890.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/213.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/244.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/715.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/188.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1729.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/103.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/57.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/378.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/399.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/392.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/424.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/906.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/292.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/537.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/800.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/183.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/382.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/23.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/941.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/203.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/401.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1189.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/126.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/701.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/223.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/441.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/878.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/803.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/32.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/356.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/116.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/505.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/929.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/927.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/544.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/546.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/300.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/249.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/856.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/165.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/922.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/866.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/321.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/70.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/818.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/887.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/523.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/764.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/818.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/349.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/290.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/53.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/610.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1876.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/511.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/550.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/236.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1823.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/261.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/285.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/173.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/580.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1446.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/567.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/301.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/655.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/363.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/100.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/855.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/800.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/785.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/122.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/537.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1843.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/707.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/104.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/286.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/791.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1244.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/264.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/635.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/930.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/732.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/168.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/847.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/363.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/919.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1083.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/119.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1811.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/176.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/601.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/351.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/54.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/122.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/318.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/884.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/641.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/28.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1072.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/704.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/24.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/788.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1650.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/804.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/449.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/973.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/834.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/223.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/944.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/156.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/533.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/729.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/239.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/169.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/460.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/592.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/182.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/278.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1799.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/386.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/416.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/236.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/409.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/565.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/891.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/129.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/276.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/76.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/733.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/604.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/395.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/776.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/302.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/47.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/196.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/941.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/5.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/318.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/875.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/33.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/96.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1206.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/74.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1134.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/595.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/323.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/439.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1590.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/482.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/76.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/426.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/861.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/230.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/872.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/541.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/548.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/823.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/57.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/60.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/896.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/946.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/785.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/527.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/769.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1005.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/868.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1198.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/778.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/435.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/282.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/791.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/232.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1851.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/764.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/249.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1107.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/670.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/507.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1609.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/216.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/239.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/756.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/91.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/77.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1633.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/832.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/60.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/625.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/935.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/1.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/621.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/319.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/648.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/806.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/394.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/14.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1424.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1878.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/851.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/720.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1031.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/133.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/372.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/206.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/677.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1308.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/29.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/753.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/397.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/898.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/520.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/289.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/428.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/536.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/529.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/905.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/51.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/56.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/792.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/890.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/722.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1488.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1101.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/837.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/149.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/327.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/750.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/654.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/518.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/918.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/400.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/791.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/465.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/302.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1552.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/901.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1412.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/905.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/158.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/796.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/820.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/90.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/30.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/954.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/309.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/891.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/871.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/252.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/184.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/968.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/49.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1362.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/977.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/170.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/850.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/229.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/165.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/16.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/794.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/620.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/675.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/299.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/630.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/955.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/560.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/658.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1802.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/276.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/411.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/305.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/142.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/549.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/717.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/811.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/1521.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/193.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/513.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/9.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/254.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/772.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/218.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/paper/529.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/251.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/771.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/255.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/scissors/192.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/rock/360.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/examples/customization/.training-data/rps_data_sample/none/744.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:41:38.093868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:38.093994: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.210152: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.210239: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.210299: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.210362: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.702777: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.702867: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.702953: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "sys/bus/pci/devices/0000:43:00.0/numa_nodeal_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.703091: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.703149: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.726266: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.726472: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.726548: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.726559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-13 11:41:41.726623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.726632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 1, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-13 11:41:41.726722: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:08:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.730186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9597 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:08:00.0, compute capability: 6.1\n",
      "2025-02-13 11:41:41.733705: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:43:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-13 11:41:41.733758: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 9598 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:43:00.0, compute capability: 6.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder\n",
      "INFO:tensorflow:Load valid hands with size: 473, num_label: 4, labels: none,paper,rock,scissors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load valid hands with size: 473, num_label: 4, labels: none,paper,rock,scissors.\n"
     ]
    }
   ],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path,\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndTh_ZyEIeKV"
   },
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAXWc3bv8hpe"
   },
   "source": [
    "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.920041400Z"
    },
    "id": "yk0UiRB6NZrb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hand_embedding (InputLayer  [(None, 128)]             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_15 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_15 (ReLU)             (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_15 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 64)                8256      \n",
      " 0 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_16 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_16 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_16 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 32)                2080      \n",
      " 1 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_17 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_17 (ReLU)             (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_17 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 4)                 132       \n",
      " out (Dense)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11364 (44.39 KB)\n",
      "Trainable params: 10916 (42.64 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from ./.model/exported_model2/epoch_models/model-0001\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/usr/lib/python3.10/_collections_abc.py:824\u001b[0m, in \u001b[0;36mMapping.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/utils/object_identity.py:139\u001b[0m, in \u001b[0;36mObjectIdentityDictionary.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_storage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wrap_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: <_ObjectIdentityWrapper wrapping <tf.Variable 'total:0' shape=() dtype=float32, numpy=0.0>>",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m model_options \u001b[38;5;241m=\u001b[39m gesture_recognizer\u001b[38;5;241m.\u001b[39mModelOptions(\n\u001b[1;32m     10\u001b[0m     layer_widths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m32\u001b[39m]  \u001b[38;5;66;03m# Add intermediate layers\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m options \u001b[38;5;241m=\u001b[39m gesture_recognizer\u001b[38;5;241m.\u001b[39mGestureRecognizerOptions(\n\u001b[1;32m     13\u001b[0m     hparams\u001b[38;5;241m=\u001b[39mhparams,\n\u001b[1;32m     14\u001b[0m     model_options\u001b[38;5;241m=\u001b[39mmodel_options,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGestureRecognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py:94\u001b[0m, in \u001b[0;36mGestureRecognizer.create\u001b[0;34m(cls, train_data, validation_data, options)\u001b[0m\n\u001b[1;32m     88\u001b[0m   options\u001b[38;5;241m.\u001b[39mhparams \u001b[38;5;241m=\u001b[39m hp\u001b[38;5;241m.\u001b[39mHParams()\n\u001b[1;32m     90\u001b[0m gesture_recognizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\n\u001b[1;32m     91\u001b[0m     label_names\u001b[38;5;241m=\u001b[39mtrain_data\u001b[38;5;241m.\u001b[39mlabel_names,\n\u001b[1;32m     92\u001b[0m     model_options\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mmodel_options,\n\u001b[1;32m     93\u001b[0m     hparams\u001b[38;5;241m=\u001b[39moptions\u001b[38;5;241m.\u001b[39mhparams)\n\u001b[0;32m---> 94\u001b[0m \u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_and_train_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m gesture_recognizer\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/mediapipe_model_maker/python/vision/gesture_recognizer/gesture_recognizer.py:109\u001b[0m, in \u001b[0;36mGestureRecognizer._create_and_train_model\u001b[0;34m(self, train_data, validation_data)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates and trains the model.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \n\u001b[1;32m    104\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m  train_data: Training data.\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m  validation_data: Validation data.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model()\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_checkpoint_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/mediapipe_model_maker/python/core/tasks/classifier.py:120\u001b[0m, in \u001b[0;36mClassifier._train_model\u001b[0;34m(self, train_data, validation_data, preprocessor, checkpoint_path)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    119\u001b[0m   steps_per_epoch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hparams\u001b[38;5;241m.\u001b[39msteps_per_epoch\n\u001b[0;32m--> 120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_callbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1807\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1799\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1800\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1801\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1804\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1805\u001b[0m ):\n\u001b[1;32m   1806\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1807\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1809\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:832\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    829\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 832\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    834\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    835\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:888\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 888\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    890\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[1;32m    891\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[1;32m    892\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:695\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_scoped_tracing_options(\n\u001b[1;32m    691\u001b[0m     variable_capturing_scope,\n\u001b[1;32m    692\u001b[0m     tracing_compilation\u001b[38;5;241m.\u001b[39mScopeType\u001b[38;5;241m.\u001b[39mVARIABLE_CREATION,\n\u001b[1;32m    693\u001b[0m )\n\u001b[1;32m    694\u001b[0m \u001b[38;5;66;03m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m--> 695\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_variable_creation_fn \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrace_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[1;32m    700\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:178\u001b[0m, in \u001b[0;36mtrace_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    175\u001b[0m     args \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39minput_signature\n\u001b[1;32m    176\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 178\u001b[0m   concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    179\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mbind_graph_to_function:\n\u001b[1;32m    183\u001b[0m   concrete_function\u001b[38;5;241m.\u001b[39m_garbage_collector\u001b[38;5;241m.\u001b[39mrelease()  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:283\u001b[0m, in \u001b[0;36m_maybe_define_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    282\u001b[0m   target_func_type \u001b[38;5;241m=\u001b[39m lookup_func_type\n\u001b[0;32m--> 283\u001b[0m concrete_function \u001b[38;5;241m=\u001b[39m \u001b[43m_create_concrete_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    284\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_func_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlookup_func_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtracing_options\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    288\u001b[0m   tracing_options\u001b[38;5;241m.\u001b[39mfunction_cache\u001b[38;5;241m.\u001b[39madd(\n\u001b[1;32m    289\u001b[0m       concrete_function, current_func_context\n\u001b[1;32m    290\u001b[0m   )\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:310\u001b[0m, in \u001b[0;36m_create_concrete_function\u001b[0;34m(function_type, type_context, func_graph, tracing_options)\u001b[0m\n\u001b[1;32m    303\u001b[0m   placeholder_bound_args \u001b[38;5;241m=\u001b[39m function_type\u001b[38;5;241m.\u001b[39mplaceholder_arguments(\n\u001b[1;32m    304\u001b[0m       placeholder_context\n\u001b[1;32m    305\u001b[0m   )\n\u001b[1;32m    307\u001b[0m disable_acd \u001b[38;5;241m=\u001b[39m tracing_options\u001b[38;5;241m.\u001b[39mattributes \u001b[38;5;129;01mand\u001b[39;00m tracing_options\u001b[38;5;241m.\u001b[39mattributes\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    308\u001b[0m     attributes_lib\u001b[38;5;241m.\u001b[39mDISABLE_ACD, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    309\u001b[0m )\n\u001b[0;32m--> 310\u001b[0m traced_func_graph \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtracing_options\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mplaceholder_bound_args\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    315\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    316\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    317\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_control_dependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdisable_acd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    318\u001b[0m \u001b[43m    \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction_type_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_arg_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_placeholders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    322\u001b[0m transform\u001b[38;5;241m.\u001b[39mapply_func_graph_transforms(traced_func_graph)\n\u001b[1;32m    324\u001b[0m graph_capture_container \u001b[38;5;241m=\u001b[39m traced_func_graph\u001b[38;5;241m.\u001b[39mfunction_captures\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/framework/func_graph.py:1059\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, create_placeholders)\u001b[0m\n\u001b[1;32m   1056\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m   1058\u001b[0m _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[0;32m-> 1059\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mpython_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfunc_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m variable_utils\u001b[38;5;241m.\u001b[39mconvert_variables_to_tensors(func_outputs)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:598\u001b[0m, in \u001b[0;36mFunction._generate_scoped_tracing_options.<locals>.wrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    595\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[1;32m    596\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[1;32m    597\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[0;32m--> 598\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mweak_wrapped_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__wrapped__\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/autograph_util.py:41\u001b[0m, in \u001b[0;36mpy_func_from_autograph.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls a converted version of original_func.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 41\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m      \u001b[49m\u001b[43moriginal_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m      \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m      \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConversionOptions\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m          \u001b[49m\u001b[43mrecursive\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m          \u001b[49m\u001b[43moptional_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m          \u001b[49m\u001b[43muser_requested\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m      \u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m     51\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:439\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    438\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 439\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mconverted_f\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43meffective_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m     result \u001b[38;5;241m=\u001b[39m converted_f(\u001b[38;5;241m*\u001b[39meffective_args)\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file4n5dyz_v.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m \u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_function\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:460\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1384\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1380\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1381\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1382\u001b[0m     )\n\u001b[1;32m   1383\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1384\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1385\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1386\u001b[0m     outputs,\n\u001b[1;32m   1387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1388\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1389\u001b[0m )\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:1681\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1676\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1677\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1678\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1679\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1680\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1681\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:3271\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3269\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   3270\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 3271\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/distribute/distribute_lib.py:4069\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   4067\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   4068\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 4069\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:690\u001b[0m, in \u001b[0;36mconvert.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    689\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m conversion_ctx:\n\u001b[0;32m--> 690\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m    692\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:377\u001b[0m, in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _call_unconverted(f, args, kwargs, options)\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39muser_requested \u001b[38;5;129;01mand\u001b[39;00m conversion\u001b[38;5;241m.\u001b[39mis_allowlisted(f):\n\u001b[0;32m--> 377\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_call_unconverted\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[39;00m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;66;03m# call conversion from generated code while in nonrecursive mode. In that\u001b[39;00m\n\u001b[1;32m    381\u001b[0m \u001b[38;5;66;03m# case we evidently don't want to recurse, but we still have to convert\u001b[39;00m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;66;03m# things like builtins.\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m options\u001b[38;5;241m.\u001b[39minternal_convert_user_code:\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/autograph/impl/api.py:459\u001b[0m, in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__self__\u001b[39m\u001b[38;5;241m.\u001b[39mcall(args, kwargs)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 459\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1373\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1372\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1373\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1374\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1155\u001b[0m, in \u001b[0;36mModel.train_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;66;03m# Run backwards pass.\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mminimize(loss, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainable_variables, tape\u001b[38;5;241m=\u001b[39mtape)\n\u001b[0;32m-> 1155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.compute_metrics\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1214\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Update metric states and collect all metrics to be returned.\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \n\u001b[1;32m   1216\u001b[0m \u001b[38;5;124;03mSubclasses can optionally override this method to provide custom metric\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03m  `{'loss': 0.2, 'accuracy': 0.7}`.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1248\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m x  \u001b[38;5;66;03m# The default implementation does not use `x`.\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompiled_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metrics_result()\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/compile_utils.py:592\u001b[0m, in \u001b[0;36mMetricsContainer.update_state\u001b[0;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[1;32m    589\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conform_to_outputs(y_pred, sample_weight)\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_built:\n\u001b[0;32m--> 592\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(y_pred)\n\u001b[1;32m    595\u001b[0m y_true \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(y_true) \u001b[38;5;28;01mif\u001b[39;00m y_true \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m []\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/compile_utils.py:498\u001b[0m, in \u001b[0;36mMetricsContainer.build\u001b[0;34m(self, y_pred, y_true)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weighted_metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mlist_to_tuple(\n\u001b[1;32m    493\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weighted_metrics\n\u001b[1;32m    494\u001b[0m )\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Convert to `Metric` objects, potentially disambiguating based on\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;66;03m# output properties.\u001b[39;00m\n\u001b[0;32m--> 498\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_metric_objects\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metrics\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_weighted_metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mmap_structure_up_to(\n\u001b[1;32m    502\u001b[0m     y_pred,\n\u001b[1;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metric_objects,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m     y_pred,\n\u001b[1;32m    507\u001b[0m )\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten_up_to(\n\u001b[1;32m    510\u001b[0m     y_pred, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metrics, check_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    511\u001b[0m )\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:1037\u001b[0m, in \u001b[0;36mmap_structure_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1037\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_structure_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnest_util\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mModality\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCORE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1039\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Discards the path arg.\u001b[39;49;00m\n\u001b[1;32m   1041\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1042\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1043\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1650\u001b[0m, in \u001b[0;36mmap_structure_up_to\u001b[0;34m(modality, shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1576\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m   1577\u001b[0m \n\u001b[1;32m   1578\u001b[0m \u001b[38;5;124;03mThe `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;124;03m  `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1648\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mCORE:\n\u001b[0;32m-> 1650\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_tf_core_map_structure_with_tuple_paths_up_to\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m      \u001b[49m\u001b[43mshallow_tree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modality \u001b[38;5;241m==\u001b[39m Modality\u001b[38;5;241m.\u001b[39mDATA:\n\u001b[1;32m   1654\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _tf_data_map_structure_up_to(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1699\u001b[0m, in \u001b[0;36m_tf_core_map_structure_with_tuple_paths_up_to\u001b[0;34m(shallow_tree, func, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1684\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1685\u001b[0m     _tf_core_flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m         shallow_tree,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m   1692\u001b[0m )\n\u001b[1;32m   1693\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1694\u001b[0m     path\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _tf_core_yield_flat_up_to(\n\u001b[1;32m   1696\u001b[0m         shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn\n\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m )\n\u001b[0;32m-> 1699\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   1700\u001b[0m     func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1701\u001b[0m ]\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1703\u001b[0m     structure\u001b[38;5;241m=\u001b[39mshallow_tree,\n\u001b[1;32m   1704\u001b[0m     flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1705\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1706\u001b[0m )\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest_util.py:1700\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1684\u001b[0m flat_value_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1685\u001b[0m     _tf_core_flatten_up_to(  \u001b[38;5;66;03m# pylint: disable=g-complex-comprehension\u001b[39;00m\n\u001b[1;32m   1686\u001b[0m         shallow_tree,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1691\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_tree \u001b[38;5;129;01min\u001b[39;00m inputs\n\u001b[1;32m   1692\u001b[0m )\n\u001b[1;32m   1693\u001b[0m flat_path_gen \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1694\u001b[0m     path\n\u001b[1;32m   1695\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path, _ \u001b[38;5;129;01min\u001b[39;00m _tf_core_yield_flat_up_to(\n\u001b[1;32m   1696\u001b[0m         shallow_tree, inputs[\u001b[38;5;241m0\u001b[39m], is_nested_fn\n\u001b[1;32m   1697\u001b[0m     )\n\u001b[1;32m   1698\u001b[0m )\n\u001b[1;32m   1699\u001b[0m results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m-> 1700\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_path_gen, \u001b[38;5;241m*\u001b[39mflat_value_gen)\n\u001b[1;32m   1701\u001b[0m ]\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _tf_core_pack_sequence_as(\n\u001b[1;32m   1703\u001b[0m     structure\u001b[38;5;241m=\u001b[39mshallow_tree,\n\u001b[1;32m   1704\u001b[0m     flat_sequence\u001b[38;5;241m=\u001b[39mresults,\n\u001b[1;32m   1705\u001b[0m     expand_composites\u001b[38;5;241m=\u001b[39mexpand_composites,\n\u001b[1;32m   1706\u001b[0m )\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/tensorflow/python/util/nest.py:1040\u001b[0m, in \u001b[0;36mmap_structure_up_to.<locals>.<lambda>\u001b[0;34m(_, *values)\u001b[0m\n\u001b[1;32m    963\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__internal__.nest.map_structure_up_to\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    964\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmap_structure_up_to\u001b[39m(shallow_tree, func, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    965\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a function or op to a number of partially flattened inputs.\u001b[39;00m\n\u001b[1;32m    966\u001b[0m \n\u001b[1;32m    967\u001b[0m \u001b[38;5;124;03m  The `inputs` are flattened up to `shallow_tree` before being mapped.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    `shallow_tree`.\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m nest_util\u001b[38;5;241m.\u001b[39mmap_structure_up_to(\n\u001b[1;32m   1038\u001b[0m       nest_util\u001b[38;5;241m.\u001b[39mModality\u001b[38;5;241m.\u001b[39mCORE,\n\u001b[1;32m   1039\u001b[0m       shallow_tree,\n\u001b[0;32m-> 1040\u001b[0m       \u001b[38;5;28;01mlambda\u001b[39;00m _, \u001b[38;5;241m*\u001b[39mvalues: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m,  \u001b[38;5;66;03m# Discards the path arg.\u001b[39;00m\n\u001b[1;32m   1041\u001b[0m       \u001b[38;5;241m*\u001b[39minputs,\n\u001b[1;32m   1042\u001b[0m       \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1043\u001b[0m   )\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/compile_utils.py:646\u001b[0m, in \u001b[0;36mMetricsContainer._get_metric_objects\u001b[0;34m(self, metrics, y_t, y_p)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert user-supplied metrics to `Metric` objects.\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(metrics)\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_metric_object(m, y_t, y_p) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics]\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/compile_utils.py:646\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convert user-supplied metrics to `Metric` objects.\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m metrics \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnest\u001b[38;5;241m.\u001b[39mflatten(metrics)\n\u001b[0;32m--> 646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_metric_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_p\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics]\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/compile_utils.py:705\u001b[0m, in \u001b[0;36mMetricsContainer._get_metric_object\u001b[0;34m(self, metric, y_t, y_p)\u001b[0m\n\u001b[1;32m    700\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m metric_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    701\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    702\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric should be a callable, received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    703\u001b[0m             )\n\u001b[0;32m--> 705\u001b[0m     metric_obj \u001b[38;5;241m=\u001b[39m \u001b[43mmetrics_mod\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMeanMetricWrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmetric_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mesh\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m metric_obj\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/dtensor/utils.py:144\u001b[0m, in \u001b[0;36minject_mesh.<locals>._wrap_function\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;241m=\u001b[39m mesh\n\u001b[0;32m--> 144\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/metrics/base_metric.py:682\u001b[0m, in \u001b[0;36mMeanMetricWrapper.__init__\u001b[0;34m(self, fn, name, dtype, **kwargs)\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[38;5;129m@dtensor_utils\u001b[39m\u001b[38;5;241m.\u001b[39minject_mesh\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 682\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    683\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn \u001b[38;5;241m=\u001b[39m fn\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fn_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/dtensor/utils.py:144\u001b[0m, in \u001b[0;36minject_mesh.<locals>._wrap_function\u001b[0;34m(instance, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mesh \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     instance\u001b[38;5;241m.\u001b[39m_mesh \u001b[38;5;241m=\u001b[39m mesh\n\u001b[0;32m--> 144\u001b[0m \u001b[43minit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/metrics/base_metric.py:645\u001b[0m, in \u001b[0;36mMean.__init__\u001b[0;34m(self, name, dtype)\u001b[0m\n\u001b[1;32m    643\u001b[0m \u001b[38;5;129m@dtensor_utils\u001b[39m\u001b[38;5;241m.\u001b[39minject_mesh\n\u001b[1;32m    644\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 645\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    646\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mReduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mWEIGHTED_MEAN\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    647\u001b[0m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    648\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/metrics/base_metric.py:462\u001b[0m, in \u001b[0;36mReduce.__init__\u001b[0;34m(self, reduction, name, dtype)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(name\u001b[38;5;241m=\u001b[39mname, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction \u001b[38;5;241m=\u001b[39m reduction\n\u001b[0;32m--> 462\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtotal\u001b[49m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal\u001b[39m\u001b[38;5;124m\"\u001b[39m, initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reduction \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[1;32m    464\u001b[0m     metrics_utils\u001b[38;5;241m.\u001b[39mReduction\u001b[38;5;241m.\u001b[39mSUM_OVER_BATCH_SIZE,\n\u001b[1;32m    465\u001b[0m     metrics_utils\u001b[38;5;241m.\u001b[39mReduction\u001b[38;5;241m.\u001b[39mWEIGHTED_MEAN,\n\u001b[1;32m    466\u001b[0m ]:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcount \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcount\u001b[39m\u001b[38;5;124m\"\u001b[39m, initializer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/keras/src/engine/base_layer.py:3172\u001b[0m, in \u001b[0;36mLayer.__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m   3167\u001b[0m value \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39m__internal__\u001b[38;5;241m.\u001b[39mtracking\u001b[38;5;241m.\u001b[39msticky_attribute_assignment(\n\u001b[1;32m   3168\u001b[0m     trackable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, value\u001b[38;5;241m=\u001b[39mvalue, name\u001b[38;5;241m=\u001b[39mname\n\u001b[1;32m   3169\u001b[0m )\n\u001b[1;32m   3171\u001b[0m reference_counts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_reference_counts\n\u001b[0;32m-> 3172\u001b[0m reference_counts[value] \u001b[38;5;241m=\u001b[39m \u001b[43mreference_counts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3174\u001b[0m \u001b[38;5;66;03m# When replacing an existing tf.Variable with a new one, we want to\u001b[39;00m\n\u001b[1;32m   3175\u001b[0m \u001b[38;5;66;03m# check its existing position in the\u001b[39;00m\n\u001b[1;32m   3176\u001b[0m \u001b[38;5;66;03m# self._trainable/non_trainable_variable, so that we can put it back to\u001b[39;00m\n\u001b[1;32m   3177\u001b[0m \u001b[38;5;66;03m# the original position.\u001b[39;00m\n\u001b[1;32m   3178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, tf\u001b[38;5;241m.\u001b[39mVariable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   3179\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, \u001b[38;5;28;01mNone\u001b[39;00m), tf\u001b[38;5;241m.\u001b[39mVariable\n\u001b[1;32m   3180\u001b[0m ):\n",
      "File \u001b[0;32m/usr/lib/python3.10/_collections_abc.py:824\u001b[0m, in \u001b[0;36mMapping.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD.get(k[,d]) -> D[k] if k in D, else d.  d defaults to None.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    826\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m default\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "!mkdir -p \"./.model\"\n",
    "hparams = gesture_recognizer.HParams(\n",
    "    export_dir=\"./.model/exported_model2\",\n",
    "    learning_rate=0.0005,\n",
    "    batch_size=4,\n",
    "    epochs=10, \n",
    "    lr_decay=0.99,\n",
    ")\n",
    "model_options = gesture_recognizer.ModelOptions(\n",
    "    layer_widths=[64, 32]  # Add intermediate layers\n",
    ")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(\n",
    "    hparams=hparams,\n",
    "    model_options=model_options,\n",
    ")\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nED7mdIO9YS6"
   },
   "source": [
    "**Evaluate the model performance**\n",
    "\n",
    "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.922018300Z"
    },
    "id": "OdOqllqx9YKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 4s 10ms/step - loss: 0.2174 - categorical_accuracy: 0.8542\n",
      "Test loss:0.21735136210918427, Test accuracy:0.8541666865348816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:51:46.674421: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 7423246796892285392\n",
      "2025-02-13 11:51:46.674529: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10203139980716914487\n",
      "2025-02-13 11:51:46.674547: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 9006436443737671257\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJLramjy9gvy"
   },
   "source": [
    "**Export to Tensorflow Lite Model**\n",
    "\n",
    "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.923532900Z"
    },
    "id": "fmNaFXytijVg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpnm2fi12k/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnm2fi12k/saved_model/assets\n",
      "2025-02-13 11:52:48.544326: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-02-13 11:52:48.544383: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-02-13 11:52:48.544604: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpnm2fi12k/saved_model\n",
      "2025-02-13 11:52:48.547054: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-02-13 11:52:48.547080: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpnm2fi12k/saved_model\n",
      "2025-02-13 11:52:48.553456: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-02-13 11:52:48.599270: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpnm2fi12k/saved_model\n",
      "2025-02-13 11:52:48.617103: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 72500 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 8, Total Ops 17, % non-converted = 47.06 %\n",
      " * 8 ARITH ops\n",
      "\n",
      "- arith.constant:    8 occurrences  (f32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access 'exported_model': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "model.export_model()\n",
    "!ls exported_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.927527100Z"
    },
    "id": "7yfN_47qjjOC"
   },
   "outputs": [],
   "source": [
    "# files.download('exported_model/gesture_recognizer.task')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulqyNGmTCKeU"
   },
   "source": [
    "## Run the model on-device\n",
    "\n",
    "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1tiLGGRcvhy"
   },
   "source": [
    "## Hyperparameters {:#hyperparameters}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1UMEG85hQL_"
   },
   "source": [
    "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
    "\n",
    "`ModelOptions` has one customizable parameter that affects accuracy:\n",
    "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
    "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
    "\n",
    "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
    "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
    "* `batch_size`: Batch size for training. Defaults to 2.\n",
    "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
    "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
    "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
    "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
    "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
    "\n",
    "Additional `HParams` parameter that does not affect model accuracy:\n",
    "* `export_dir`: The location of the model checkpoint files and exported model files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psvVZeSYBLfV"
   },
   "source": [
    "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.928527700Z"
    },
    "id": "CxMOI8o6iNLu"
   },
   "outputs": [],
   "source": [
    "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
    "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
    "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
    "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cekuTJiBbv9"
   },
   "source": [
    "Evaluate the newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.930528600Z"
    },
    "id": "RRH96bm-BbAo"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_2.evaluate(test_data)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
