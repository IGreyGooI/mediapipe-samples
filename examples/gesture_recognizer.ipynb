{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-9BpIlqAZci"
   },
   "source": [
    "Project: /mediapipe/_project.yaml\n",
    "Book: /mediapipe/_book.yaml\n",
    "\n",
    "<link rel=\"stylesheet\" href=\"/mediapipe/site.css\">\n",
    "\n",
    "# Hand gesture recognition model customization guide\n",
    "\n",
    "<table align=\"left\" class=\"buttons\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/colab-logo-32px_1920.png\" alt=\"Colab logo\"> Run in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "\n",
    "  <td>\n",
    "    <a href=\"https://github.com/googlesamples/mediapipe/blob/main/examples/customization/gesture_recognizer.ipynb\" target=\"_blank\">\n",
    "      <img src=\"https://developers.google.com/static/mediapipe/solutions/customization/github-logo-32px_1920.png\" alt=\"GitHub logo\">\n",
    "      View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T19:54:19.922801600Z",
     "start_time": "2025-02-12T19:54:19.735323Z"
    },
    "id": "JO1GUwC1_T2x"
   },
   "outputs": [],
   "source": [
    "#@title License information\n",
    "# Copyright 2023 The MediaPipe Authors.\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "#\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wFBcmjzf0JLE"
   },
   "source": [
    "The MediaPipe Model Maker package is a low-code solution for customizing on-device machine learning (ML) Models.\n",
    "\n",
    "This notebook shows the end-to-end process of customizing a gesture recognizer model for recognizing some common hand gestures in the [HaGRID](https://www.kaggle.com/datasets/innominate817/hagrid-sample-30k-384p) dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YGM0PT490LiR"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVVxZNfo0M0y"
   },
   "source": [
    "Install the MediaPipe Model Maker package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T20:46:22.488329400Z",
     "start_time": "2025-02-12T20:46:11.986728600Z"
    },
    "id": "6DBLRE-fqlO5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.10/site-packages (22.0.2)\n",
      "Collecting pip\n",
      "  Using cached pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.0.2\n",
      "    Uninstalling pip-22.0.2:\n",
      "      Successfully uninstalled pip-22.0.2\n",
      "Successfully installed pip-25.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: mediapipe-model-maker in ./.venv/lib/python3.10/site-packages (0.2.1.4)\n",
      "Requirement already satisfied: absl-py in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (1.4.0)\n",
      "Requirement already satisfied: mediapipe>=0.10.0 in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.10.21)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (1.26.4)\n",
      "Requirement already satisfied: opencv-python in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: tensorflow<2.16,>=2.10 in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.1)\n",
      "Requirement already satisfied: tensorflow-addons in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.23.0)\n",
      "Requirement already satisfied: tensorflow-datasets in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (4.9.7)\n",
      "Requirement already satisfied: tensorflow-hub in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.16.1)\n",
      "Requirement already satisfied: tensorflow-model-optimization<0.8.0 in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (0.7.5)\n",
      "Requirement already satisfied: tensorflow-text in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: tf-models-official<2.16.0,>=2.13.2 in ./.venv/lib/python3.10/site-packages (from mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (25.1.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (25.2.10)\n",
      "Requirement already satisfied: jax in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.34)\n",
      "Requirement already satisfied: jaxlib in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.4.34)\n",
      "Requirement already satisfied: matplotlib in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (3.10.0)\n",
      "Requirement already satisfied: opencv-contrib-python in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: protobuf<5,>=4.25.3 in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (4.25.6)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.5.1)\n",
      "Requirement already satisfied: sentencepiece in ./.venv/lib/python3.10/site-packages (from mediapipe>=0.10.0->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.6.3)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.12.1)\n",
      "Requirement already satisfied: libclang>=13.0.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.4.0)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (24.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (59.6.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (4.12.2)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.37.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.70.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in ./.venv/lib/python3.10/site-packages (from tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.15.0)\n",
      "Requirement already satisfied: dm-tree~=0.1.1 in ./.venv/lib/python3.10/site-packages (from tensorflow-model-optimization<0.8.0->mediapipe-model-maker) (0.1.9)\n",
      "Requirement already satisfied: Cython in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.0.12)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (11.1.0)\n",
      "Requirement already satisfied: gin-config in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.0)\n",
      "Requirement already satisfied: google-api-python-client>=1.6.7 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.160.0)\n",
      "Requirement already satisfied: immutabledict in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.2.1)\n",
      "Requirement already satisfied: kaggle>=1.3.9 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.6.17)\n",
      "Requirement already satisfied: oauth2client in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.3)\n",
      "Requirement already satisfied: opencv-python-headless in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.11.0.86)\n",
      "Requirement already satisfied: pandas>=0.22.0 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.2.3)\n",
      "Requirement already satisfied: psutil>=5.4.3 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.1.1)\n",
      "Requirement already satisfied: py-cpuinfo>=3.3.0 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (9.0.0)\n",
      "Requirement already satisfied: pycocotools in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.0.8)\n",
      "Requirement already satisfied: pyyaml>=6.0.0 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.0.2)\n",
      "Requirement already satisfied: sacrebleu in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.5.1)\n",
      "Requirement already satisfied: scipy>=0.19.1 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.15.1)\n",
      "Requirement already satisfied: seqeval in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.2.2)\n",
      "Requirement already satisfied: tf-slim>=1.1.0 in ./.venv/lib/python3.10/site-packages (from tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.1.0)\n",
      "Requirement already satisfied: tf-keras>=2.14.1 in ./.venv/lib/python3.10/site-packages (from tensorflow-hub->mediapipe-model-maker) (2.15.1)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in ./.venv/lib/python3.10/site-packages (from tensorflow-addons->mediapipe-model-maker) (2.13.3)\n",
      "Requirement already satisfied: click in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (8.1.8)\n",
      "Requirement already satisfied: promise in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (2.3)\n",
      "Requirement already satisfied: pyarrow in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (19.0.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (2.32.3)\n",
      "Requirement already satisfied: simple-parsing in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.1.7)\n",
      "Requirement already satisfied: tensorflow-metadata in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (1.13.1)\n",
      "Requirement already satisfied: toml in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.10.2)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (4.67.1)\n",
      "Requirement already satisfied: array-record>=0.5.0 in ./.venv/lib/python3.10/site-packages (from tensorflow-datasets->mediapipe-model-maker) (0.6.0)\n",
      "Requirement already satisfied: etils>=1.6.0 in ./.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in ./.venv/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.45.1)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (2025.2.0)\n",
      "Requirement already satisfied: importlib_resources in ./.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (6.5.2)\n",
      "Requirement already satisfied: zipp in ./.venv/lib/python3.10/site-packages (from etils[edc,enp,epath,epy,etree]>=1.6.0; python_version < \"3.11\"->tensorflow-datasets->mediapipe-model-maker) (3.21.0)\n",
      "Requirement already satisfied: httplib2<1.dev0,>=0.19.0 in ./.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.22.0)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0 in ./.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.38.0)\n",
      "Requirement already satisfied: google-auth-httplib2<1.0.0,>=0.2.0 in ./.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.2.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5 in ./.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.24.1)\n",
      "Requirement already satisfied: uritemplate<5,>=3.0.1 in ./.venv/lib/python3.10/site-packages (from google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.1.1)\n",
      "Requirement already satisfied: bleach in ./.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (6.2.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in ./.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.9.0.post0)\n",
      "Requirement already satisfied: python-slugify in ./.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in ./.venv/lib/python3.10/site-packages (from kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.10/site-packages (from pandas>=0.22.0->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2025.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests>=2.19.0->tensorflow-datasets->mediapipe-model-maker) (3.10)\n",
      "Requirement already satisfied: CFFI>=1.0 in ./.venv/lib/python3.10/site-packages (from sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (1.17.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (1.2.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.7)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in ./.venv/lib/python3.10/site-packages (from tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.1.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.venv/lib/python3.10/site-packages (from matplotlib->mediapipe>=0.10.0->mediapipe-model-maker) (3.2.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.7 in ./.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.6.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.0.5 in ./.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.1)\n",
      "Requirement already satisfied: rsa>=3.1.4 in ./.venv/lib/python3.10/site-packages (from oauth2client->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (4.9)\n",
      "Requirement already satisfied: portalocker in ./.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.1.1)\n",
      "Requirement already satisfied: regex in ./.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (2024.11.6)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in ./.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.9.0)\n",
      "Requirement already satisfied: colorama in ./.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.4.6)\n",
      "Requirement already satisfied: lxml in ./.venv/lib/python3.10/site-packages (from sacrebleu->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.3.1)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in ./.venv/lib/python3.10/site-packages (from seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.6.1)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in ./.venv/lib/python3.10/site-packages (from simple-parsing->tensorflow-datasets->mediapipe-model-maker) (0.16)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in ./.venv/lib/python3.10/site-packages (from tensorflow-metadata->tensorflow-datasets->mediapipe-model-maker) (1.67.0)\n",
      "Requirement already satisfied: pycparser in ./.venv/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe>=0.10.0->mediapipe-model-maker) (2.22)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in ./.venv/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0.dev0,>=1.31.5->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.26.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in ./.venv/lib/python3.10/site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0.dev0,>=1.32.0->google-api-python-client>=1.6.7->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (5.5.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in ./.venv/lib/python3.10/site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (2.0.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in ./.venv/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.0.2)\n",
      "Requirement already satisfied: webencodings in ./.venv/lib/python3.10/site-packages (from bleach->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in ./.venv/lib/python3.10/site-packages (from python-slugify->kaggle>=1.3.9->tf-models-official<2.16.0,>=2.13.2->mediapipe-model-maker) (1.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in ./.venv/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow<2.16,>=2.10->mediapipe-model-maker) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip\n",
    "%pip install mediapipe-model-maker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v3CvTNmB1WiY"
   },
   "source": [
    "Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-12T19:54:47.624423900Z",
     "start_time": "2025-02-12T19:54:47.571171400Z"
    },
    "id": "c74UL9oI0VKU"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 11:31:10.304877: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-13 11:31:10.304966: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-13 11:31:10.305926: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-13 11:31:10.313187: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-13 11:31:21.936882: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import files\n",
    "import os\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from mediapipe_model_maker import gesture_recognizer\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IppoENBmAuFn"
   },
   "source": [
    "## Simple End-to-End Example\n",
    "\n",
    "This end-to-end example uses Model Maker to customize a model for on-device gesture recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8fMLXTdD6tW"
   },
   "source": [
    "### Get the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7TwDFilngzjs"
   },
   "source": [
    "The dataset for gesture recognition in model maker requires the following format: `<dataset_path>/<label_name>/<img_name>.*`. In addition, one of the label names (`label_names`) must be `none`. The `none` label represents any gesture that isn't classified as one of the other gestures.\n",
    "\n",
    "This example uses a rock paper scissors dataset sample which is downloaded from GCS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.913201500Z"
    },
    "id": "6dwmyg5MnR_y"
   },
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/mediapipe-tasks/gesture_recognizer/rps_data_sample.zip -O \".training-data/rps_data_sample.zip\"\n",
    "!unzip -o \".training-data/rps_data_sample.zip\" \n",
    "dataset_path = \".training-data/rps_data_sample\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiWb9Tu3lBBI"
   },
   "source": [
    "Verify the rock paper scissors dataset by printing the labels. There should be 4 gesture labels, with one of them being the `none` gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.914474700Z"
    },
    "id": "QgadM4VDj3Y2"
   },
   "outputs": [],
   "source": [
    "print(dataset_path)\n",
    "labels = []\n",
    "for i in os.listdir(dataset_path):\n",
    "  if os.path.isdir(os.path.join(dataset_path, i)):\n",
    "    labels.append(i)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CA0o59OMjqmV"
   },
   "source": [
    "To better understand the dataset, plot a couple of example images for each gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.916574300Z"
    },
    "id": "sx8PsrwYjvgO"
   },
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 5\n",
    "\n",
    "for label in labels:\n",
    "  label_dir = os.path.join(dataset_path, label)\n",
    "  example_filenames = os.listdir(label_dir)[:NUM_EXAMPLES]\n",
    "  fig, axs = plt.subplots(1, NUM_EXAMPLES, figsize=(10,2))\n",
    "  for i in range(NUM_EXAMPLES):\n",
    "    axs[i].imshow(plt.imread(os.path.join(label_dir, example_filenames[i])))\n",
    "    axs[i].get_xaxis().set_visible(False)\n",
    "    axs[i].get_yaxis().set_visible(False)\n",
    "  fig.suptitle(f'Showing {NUM_EXAMPLES} examples for {label}')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWXwEXSXlg7d"
   },
   "source": [
    "### Run the example\n",
    "The workflow consists of 4 steps which have been separated into their own code blocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF9ArLQXIu25"
   },
   "source": [
    "**Load the dataset**\n",
    "\n",
    "Load the dataset located at `dataset_path` by using the `Dataset.from_folder` method. When loading the dataset, run the pre-packaged hand detection model from MediaPipe Hands to detect the hand landmarks from the images. Any images without detected hands are ommitted from the dataset. The resulting dataset will contain the extracted hand landmark positions from each image, rather than images themselves.\n",
    "\n",
    "The `HandDataPreprocessingParams` class contains two configurable options for the data loading process:\n",
    "* `shuffle`: A boolean controlling whether to shuffle the dataset. Defaults to true.\n",
    "* `min_detection_confidence`: A float between 0 and 1 controlling the confidence threshold for hand detection.\n",
    "\n",
    "Split the dataset: 80% for training, 10% for validation, and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.918178600Z"
    },
    "id": "aTTNZsolKXiT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1643.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/575.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/942.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1784.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1739416963.450665   20549 gl_context_egl.cc:85] Successfully initialized EGL. Major : 1 Minor: 5\n",
      "I0000 00:00:1739416963.530962   20927 gl_context.cc:369] GL version: 3.1 (OpenGL ES 3.1 Mesa 23.2.1-1ubuntu3.1~22.04.3), renderer: D3D12 (NVIDIA GeForce GTX 1080 Ti)\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1739416963.720298   20937 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739416963.738009   20941 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1739416963.819647   20956 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/730.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1851.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/764.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/116.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/223.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/229.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/504.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/811.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/613.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/56.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/143.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/596.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/832.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/234.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/785.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/179.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/753.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1072.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/918.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/764.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/666.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1488.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/896.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1381.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/505.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/818.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/868.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/435.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/275.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/57.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/929.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/320.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/850.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/589.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/559.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/431.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/728.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1747.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/30.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/800.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/318.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/57.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/537.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/65.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/387.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1811.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/901.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/930.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/673.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/216.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/293.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/50.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1712.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/672.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1552.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/529.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/495.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/72.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/45.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/423.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1000.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/269.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/630.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/598.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/126.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/428.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/236.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/720.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/37.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/680.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/927.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1005.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/546.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/791.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1814.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/941.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/670.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/122.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/847.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/142.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/90.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/448.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/441.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/771.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/625.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1823.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/957.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/184.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/946.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/427.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/482.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/717.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/77.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/898.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/604.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/449.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/955.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/47.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/955.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/230.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/190.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/74.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/549.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/631.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/393.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1501.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/28.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/595.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/617.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/756.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/194.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/866.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/476.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/18.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/33.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/192.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/614.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/803.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/406.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/879.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/592.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/426.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/792.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/305.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/620.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/208.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/238.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/734.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/378.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/550.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/942.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/654.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/206.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1031.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/186.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/318.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/568.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1340.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/182.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/855.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/394.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1189.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/744.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/834.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/541.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/614.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1041.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/706.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/590.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/300.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/954.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/750.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/507.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/165.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/339.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/732.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/213.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/594.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/278.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/722.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/309.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/136.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/363.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/656.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1033.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/409.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/823.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/91.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/1.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/887.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/935.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/657.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/851.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/372.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/798.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/176.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/658.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/545.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/327.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/376.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/paper/76.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/523.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1067.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1083.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/254.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/64.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/82.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/617.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rock/54.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/76.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1299.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/none/1424.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/149.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/306.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/scissors/119.jpg\n",
      "INFO:tensorflow:Loading image /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rps_data_sample/paper\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Image decoding failed (unknown image type): /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rps_data_sample/paper",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_folder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgesture_recognizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHandDataPreprocessingParams\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m train_data, rest_data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m0.8\u001b[39m)\n\u001b[1;32m      6\u001b[0m validation_data, test_data \u001b[38;5;241m=\u001b[39m rest_data\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;241m0.5\u001b[39m)\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py:204\u001b[0m, in \u001b[0;36mDataset.from_folder\u001b[0;34m(cls, dirname, hparams)\u001b[0m\n\u001b[1;32m    197\u001b[0m all_gesture_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    198\u001b[0m     index_by_label[os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mdirname(path))]\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m all_image_paths\n\u001b[1;32m    200\u001b[0m ]\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# Compute hand data (including local hand landmark, world hand landmark, and\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# handedness) for all the input images.\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m hand_data \u001b[38;5;241m=\u001b[39m \u001b[43m_get_hand_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mall_image_paths\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mall_image_paths\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmin_detection_confidence\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# Get a list of the valid hand landmark sample in the hand data list.\u001b[39;00m\n\u001b[1;32m    209\u001b[0m valid_indices \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    210\u001b[0m     i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(hand_data)) \u001b[38;5;28;01mif\u001b[39;00m hand_data[i] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    211\u001b[0m ]\n",
      "File \u001b[0;32m/mnt/i/runtime/mediapipe-samples-1/.venv/lib/python3.10/site-packages/mediapipe_model_maker/python/vision/gesture_recognizer/dataset.py:120\u001b[0m, in \u001b[0;36m_get_hand_data\u001b[0;34m(all_image_paths, min_detection_confidence)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m all_image_paths:\n\u001b[1;32m    119\u001b[0m   tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mlogging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoading image \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, path)\n\u001b[0;32m--> 120\u001b[0m   image \u001b[38;5;241m=\u001b[39m \u001b[43m_Image\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_from_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m   data \u001b[38;5;241m=\u001b[39m hand_landmarker\u001b[38;5;241m.\u001b[39mdetect(image)\n\u001b[1;32m    122\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _validate_data_sample(data):\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Image decoding failed (unknown image type): /mnt/i/runtime/mediapipe-samples-1/.training-data/rps_data_sample/rps_data_sample/paper"
     ]
    }
   ],
   "source": [
    "data = gesture_recognizer.Dataset.from_folder(\n",
    "    dirname=dataset_path,\n",
    "    hparams=gesture_recognizer.HandDataPreprocessingParams()\n",
    ")\n",
    "train_data, rest_data = data.split(0.8)\n",
    "validation_data, test_data = rest_data.split(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ndTh_ZyEIeKV"
   },
   "source": [
    "**Train the model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yAXWc3bv8hpe"
   },
   "source": [
    "Train the custom gesture recognizer by using the create method and passing in the training data, validation data, model options, and hyperparameters. For more information on model options and hyperparameters, see the [Hyperparameters](#hyperparameters) section below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.920041400Z"
    },
    "id": "yk0UiRB6NZrb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hand_embedding (InputLayer  [(None, 128)]             0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " batch_normalization_11 (Ba  (None, 128)               512       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_11 (ReLU)             (None, 128)               0         \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 128)               0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 64)                8256      \n",
      " 0 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_12 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " re_lu_12 (ReLU)             (None, 64)                0         \n",
      "                                                                 \n",
      " dropout_12 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 32)                2080      \n",
      " 1 (Dense)                                                       \n",
      "                                                                 \n",
      " batch_normalization_13 (Ba  (None, 32)                128       \n",
      " tchNormalization)                                               \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                 \n",
      " re_lu_13 (ReLU)             (None, 32)                0         \n",
      "                                                                 \n",
      " dropout_13 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " custom_gesture_recognizer_  (None, 4)                 132       \n",
      " out (Dense)                                                     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11364 (44.39 KB)\n",
      "Trainable params: 10916 (42.64 KB)\n",
      "Non-trainable params: 448 (1.75 KB)\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Training the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resuming from exported_model2/epoch_models/model-0020\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Detecting that an object or model or tf.train.Checkpoint is being deleted with unrestored values. See the following logs for the specific values in question. To silence these warnings, use `status.expect_partial()`. See https://www.tensorflow.org/api_docs/python/tf/train/Checkpoint#restorefor details about the status object returned by the restore function.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Value in checkpoint could not be found in the restored object: (root).optimizer._variables.8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     92/Unknown - 8s 47ms/step - loss: 0.3923 - categorical_accuracy: 0.6603"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-13 10:37:03.605293: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 10736291109410696092\n",
      "2025-02-13 10:37:07.906062: I tensorflow/core/framework/local_rendezvous.cc:421] Local rendezvous recv item cancelled. Key hash: 6622540563705298746\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94/94 [==============================] - 12s 95ms/step - loss: 0.3870 - categorical_accuracy: 0.6676 - val_loss: 0.2695 - val_categorical_accuracy: 0.8298 - lr: 1.0000e-04\n",
      "Epoch 2/20\n",
      "94/94 [==============================] - 8s 88ms/step - loss: 0.3837 - categorical_accuracy: 0.6649 - val_loss: 0.2600 - val_categorical_accuracy: 0.8298 - lr: 9.9000e-05\n",
      "Epoch 3/20\n",
      "94/94 [==============================] - 10s 101ms/step - loss: 0.3655 - categorical_accuracy: 0.6968 - val_loss: 0.2553 - val_categorical_accuracy: 0.8298 - lr: 9.8010e-05\n",
      "Epoch 4/20\n",
      "94/94 [==============================] - 11s 112ms/step - loss: 0.3224 - categorical_accuracy: 0.7154 - val_loss: 0.2564 - val_categorical_accuracy: 0.8298 - lr: 9.7030e-05\n",
      "Epoch 5/20\n",
      "94/94 [==============================] - 11s 110ms/step - loss: 0.3169 - categorical_accuracy: 0.7314 - val_loss: 0.2503 - val_categorical_accuracy: 0.8298 - lr: 9.6060e-05\n",
      "Epoch 6/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.3308 - categorical_accuracy: 0.7101 - val_loss: 0.2435 - val_categorical_accuracy: 0.8298 - lr: 9.5099e-05\n",
      "Epoch 7/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.3303 - categorical_accuracy: 0.7101 - val_loss: 0.2411 - val_categorical_accuracy: 0.8298 - lr: 9.4148e-05\n",
      "Epoch 8/20\n",
      "94/94 [==============================] - 9s 95ms/step - loss: 0.3109 - categorical_accuracy: 0.7580 - val_loss: 0.2363 - val_categorical_accuracy: 0.8085 - lr: 9.3207e-05\n",
      "Epoch 9/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: 0.3225 - categorical_accuracy: 0.7367 - val_loss: 0.2336 - val_categorical_accuracy: 0.8298 - lr: 9.2274e-05\n",
      "Epoch 10/20\n",
      "94/94 [==============================] - 8s 86ms/step - loss: 0.2759 - categorical_accuracy: 0.7660 - val_loss: 0.2317 - val_categorical_accuracy: 0.8298 - lr: 9.1352e-05\n",
      "Epoch 11/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: 0.2967 - categorical_accuracy: 0.7314 - val_loss: 0.2258 - val_categorical_accuracy: 0.8298 - lr: 9.0438e-05\n",
      "Epoch 12/20\n",
      "94/94 [==============================] - 9s 93ms/step - loss: 0.2779 - categorical_accuracy: 0.7660 - val_loss: 0.2286 - val_categorical_accuracy: 0.8085 - lr: 8.9534e-05\n",
      "Epoch 13/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: 0.3095 - categorical_accuracy: 0.7340 - val_loss: 0.2261 - val_categorical_accuracy: 0.8085 - lr: 8.8638e-05\n",
      "Epoch 14/20\n",
      "94/94 [==============================] - 9s 94ms/step - loss: 0.2693 - categorical_accuracy: 0.7713 - val_loss: 0.2269 - val_categorical_accuracy: 0.8085 - lr: 8.7752e-05\n",
      "Epoch 15/20\n",
      "94/94 [==============================] - 9s 90ms/step - loss: 0.2708 - categorical_accuracy: 0.7686 - val_loss: 0.2240 - val_categorical_accuracy: 0.8085 - lr: 8.6875e-05\n",
      "Epoch 16/20\n",
      "94/94 [==============================] - 9s 98ms/step - loss: 0.2731 - categorical_accuracy: 0.7793 - val_loss: 0.2229 - val_categorical_accuracy: 0.8085 - lr: 8.6006e-05\n",
      "Epoch 17/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: 0.2599 - categorical_accuracy: 0.7926 - val_loss: 0.2233 - val_categorical_accuracy: 0.8085 - lr: 8.5146e-05\n",
      "Epoch 18/20\n",
      "94/94 [==============================] - 8s 89ms/step - loss: 0.2579 - categorical_accuracy: 0.7793 - val_loss: 0.2198 - val_categorical_accuracy: 0.8085 - lr: 8.4294e-05\n",
      "Epoch 19/20\n",
      "94/94 [==============================] - 9s 91ms/step - loss: 0.2672 - categorical_accuracy: 0.7660 - val_loss: 0.2149 - val_categorical_accuracy: 0.8298 - lr: 8.3451e-05\n",
      "Epoch 20/20\n",
      "94/94 [==============================] - 8s 87ms/step - loss: 0.2600 - categorical_accuracy: 0.7872 - val_loss: 0.2129 - val_categorical_accuracy: 0.8298 - lr: 8.2617e-05\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to connect to the remote Jupyter Server 'http://127.0.0.1:8080/'. Verify the server is running and reachable."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "hparams = gesture_recognizer.HParams(\n",
    "    export_dir=\"exported_model2\",\n",
    "    learning_rate=0.0001,\n",
    "    batch_size=4,\n",
    "    epochs=2, \n",
    "    lr_decay=0.99,\n",
    ")\n",
    "model_options = gesture_recognizer.ModelOptions(\n",
    "    layer_widths=[64, 32]  # Add intermediate layers\n",
    ")\n",
    "options = gesture_recognizer.GestureRecognizerOptions(\n",
    "    hparams=hparams,\n",
    "    model_options=model_options,\n",
    "    dropout_rate=0.3\n",
    ")\n",
    "model = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nED7mdIO9YS6"
   },
   "source": [
    "**Evaluate the model performance**\n",
    "\n",
    "After training the model, evaluate it on a test dataset and print the loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.922018300Z"
    },
    "id": "OdOqllqx9YKy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48/48 [==============================] - 4s 11ms/step - loss: 0.1836 - categorical_accuracy: 0.8958\n",
      "Test loss:0.18363477289676666, Test accuracy:0.8958333134651184\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(test_data, batch_size=1)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJLramjy9gvy"
   },
   "source": [
    "**Export to Tensorflow Lite Model**\n",
    "\n",
    "After creating the model, convert and export it to a Tensorflow Lite model format for later use on an on-device application. The export also includes model metadata, which includes the label file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.923532900Z"
    },
    "id": "fmNaFXytijVg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing files at /tmp/model_maker/gesture_recognizer/gesture_embedder.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/palm_detection_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/hand_landmark_full.tflite\n",
      "Using existing files at /tmp/model_maker/gesture_recognizer/canned_gesture_classifier.tflite\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpyijbiftd/saved_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyijbiftd/saved_model/assets\n",
      "2025-02-13 10:40:20.658260: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:378] Ignored output_format.\n",
      "2025-02-13 10:40:20.658320: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:381] Ignored drop_control_dependency.\n",
      "2025-02-13 10:40:20.658532: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpyijbiftd/saved_model\n",
      "2025-02-13 10:40:20.660562: I tensorflow/cc/saved_model/reader.cc:51] Reading meta graph with tags { serve }\n",
      "2025-02-13 10:40:20.660580: I tensorflow/cc/saved_model/reader.cc:146] Reading SavedModel debug info (if present) from: /tmp/tmpyijbiftd/saved_model\n",
      "2025-02-13 10:40:20.666208: I tensorflow/cc/saved_model/loader.cc:233] Restoring SavedModel bundle.\n",
      "2025-02-13 10:40:20.709909: I tensorflow/cc/saved_model/loader.cc:217] Running initialization op on SavedModel bundle at path: /tmp/tmpyijbiftd/saved_model\n",
      "2025-02-13 10:40:20.726309: I tensorflow/cc/saved_model/loader.cc:316] SavedModel load for tags { serve }; Status: success: OK. Took 67777 microseconds.\n",
      "Summary on the non-converted ops:\n",
      "---------------------------------\n",
      " * Accepted dialects: tfl, builtin, func\n",
      " * Non-Converted Ops: 8, Total Ops 17, % non-converted = 47.06 %\n",
      " * 8 ARITH ops\n",
      "\n",
      "- arith.constant:    8 occurrences  (f32: 8)\n",
      "\n",
      "\n",
      "\n",
      "  (f32: 1)\n",
      "  (f32: 3)\n",
      "  (f32: 1)\n",
      "  (f32: 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best_model_weights.data-00000-of-00001\tepoch_models\t\t metadata.json\n",
      "best_model_weights.index\t\tgesture_recognizer.task\n",
      "checkpoint\t\t\t\tlogs\n"
     ]
    }
   ],
   "source": [
    "model.export_model()\n",
    "!ls exported_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.927527100Z"
    },
    "id": "7yfN_47qjjOC"
   },
   "outputs": [],
   "source": [
    "# files.download('exported_model/gesture_recognizer.task')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ulqyNGmTCKeU"
   },
   "source": [
    "## Run the model on-device\n",
    "\n",
    "To use the TFLite model for on-device usage through MediaPipe Tasks, refer to the Gesture Recognizer [overview page](https://developers.google.com/mediapipe/solutions/vision/gesture_recognizer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1tiLGGRcvhy"
   },
   "source": [
    "## Hyperparameters {:#hyperparameters}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1UMEG85hQL_"
   },
   "source": [
    "You can further customize the model using the `GestureRecognizerOptions` class, which has two optional parameters for `ModelOptions` and `HParams`. Use the `ModelOptions` class to customize parameters related to the model itself, and the `HParams` class to customize other parameters related to training and saving the model.\n",
    "\n",
    "`ModelOptions` has one customizable parameter that affects accuracy:\n",
    "* `dropout_rate`: The fraction of the input units to drop. Used in dropout layer. Defaults to 0.05.\n",
    "* `layer_widths`: A list of hidden layer widths for the gesture model. Each element in the list will create a new hidden layer with the specified width. The hidden layers are separated with BatchNorm, Dropout, and ReLU. Defaults to an empty list(no hidden layers).\n",
    "\n",
    "`HParams` has the following list of customizable parameters which affect model accuracy:\n",
    "* `learning_rate`: The learning rate to use for gradient descent training. Defaults to 0.001.\n",
    "* `batch_size`: Batch size for training. Defaults to 2.\n",
    "* `epochs`: Number of training iterations over the dataset. Defaults to 10.\n",
    "* `steps_per_epoch`: An optional integer that indicates the number of training steps per epoch. If not set, the training pipeline calculates the default steps per epoch as the training dataset size divided by batch size.\n",
    "* `shuffle`: True if the dataset is shuffled before training. Defaults to False.\n",
    "* `lr_decay`: Learning rate decay to use for gradient descent training. Defaults to 0.99.\n",
    "* `gamma`: Gamma parameter for focal loss. Defaults to 2\n",
    "\n",
    "Additional `HParams` parameter that does not affect model accuracy:\n",
    "* `export_dir`: The location of the model checkpoint files and exported model files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "psvVZeSYBLfV"
   },
   "source": [
    "For example, the following trains a new model with the dropout_rate of 0.2 and learning rate of 0.003."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.928527700Z"
    },
    "id": "CxMOI8o6iNLu"
   },
   "outputs": [],
   "source": [
    "hparams = gesture_recognizer.HParams(learning_rate=0.003, export_dir=\"exported_model_2\")\n",
    "model_options = gesture_recognizer.ModelOptions(dropout_rate=0.2)\n",
    "options = gesture_recognizer.GestureRecognizerOptions(model_options=model_options, hparams=hparams)\n",
    "model_2 = gesture_recognizer.GestureRecognizer.create(\n",
    "    train_data=train_data,\n",
    "    validation_data=validation_data,\n",
    "    options=options\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3cekuTJiBbv9"
   },
   "source": [
    "Evaluate the newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-02-12T19:54:35.930528600Z"
    },
    "id": "RRH96bm-BbAo"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model_2.evaluate(test_data)\n",
    "print(f\"Test loss:{loss}, Test accuracy:{accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "last_runtime": {
    "build_target": "",
    "kind": "local"
   },
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
